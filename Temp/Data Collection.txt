Qwertorcl

==================================================================================================================================
DATA COLLECTION   INDEX 
==================================================================================================================================
1. PERFORMANCE TUNING DATA COLLECTION
		Collecting Hanganalyze and Systemstate dumps
		Errorstack
		
2. STORAGE DATA COLLECTION 
		STORAGE DATA COLLECTION ASM
		STORAGE DATA COLLECTION DNFS

		
3. REAL APPLICATION CLUSTER
		REAL APPLICATION CLUSTER 
		REAL APPLICATION CLUSTER DATABASE STARTUP
		REAL APPLICATION CLUSTER DATABASE ADMIN
		
4. GENERIC DATA COLLECTION
        GENERIC DATA COLLECTION ASM
		GENERIC DATA COLLECTION STORAGE
		GENERIC DATA COLLECTION REAL APPLICATION CLUSTER
		
5. INSTALL PATCH UPGRADE 
		
		
6. OS TRACING AND DEBUG TOOL 
		OS TRACING STRACE TRUSS TRUC
		

=========================================================================================================================================================
########################################################################################################################################################
=========================================================================================================================================================
=========================================================================================================================================================
PERFORMANCE TUNING DATA COLLECTION
===========================================



1. Take the hanganalyze and systemstatedump from the database while you are facing the issue and upload it to this SR.

		=============================================
		Collecting Hanganalyze and Systemstate dumps
		=============================================

		Using SQL*Plus connect as SYSDBA using the following command:

		sqlplus " / as sysdba"

		If there are problems making this connection then in 10gR2 and above, the sqlplus "preliminary connection" can be used :

		sqlplus -prelim " / as sysdba" (in 11.2.0.2 hanganalyze will not produce output with prelim connection)

		Hanganalyze in RAC
		==================
		SQL> oradebug setmypid
		SQL> oradebug unlimit
		SQL> oradebug setinst all
		SQL> oradebug -g all hanganalyze 3
		< wait for 2 min>
		SQL> oradebug -g all hanganalyze 3
		< wait for 2 min>
		SQL> oradebug -g all hanganalyze 3
		SQL> oradebug tracefile_name

		Systemstate in RAC
		==================
		SQL> oradebug setmypid
		SQL> oradebug unlimit
		SQL> oradebug setinst all
		SQL> oradebug -g all dump systemstate 267
		< wait for 2 min>
		SQL> oradebug -g all dump systemstate 267
		< wait for 2 min>
		SQL> oradebug -g all dump systemstate 267
		SQL> oradebug tracefile_name


2. Also generate 3 errorstack trace for the blocking session as follows and upload it to this SR

		++ Find the OS pid for the oracle session opened for sql.
		select spid from v$process where addr=(select paddr from v$session where sid=<blocking session id>);
		This will give the OS pid of the oracle session.

		Errorstack
		============
		SQL> connect / as sysdba
		SQL> oradebug setospid <spid>
		SQL> oradebug dump errorstack 3
		wait for 2 min
		SQL> oradebug dump errorstack 3
		SQL> exit




--
SQL 
SQLT 215187.1
224270.1
sanman.krishnamurthy

1] EXECTUION PLAN CHANGE - bug fix can cause these [OFE - optimiser_ parameter to previous version ]
   10046, AWR , sqlt, v$active session history 
2] Data Change
   10046 AWR ,sqlt,
3] Resource 
   OS watcher AWR

---
1] - Please upload optach lsinventry -detail
 
2] - Identify a couple of session where the "ipc send completion sync" is 
   occuring..
 
 select SPID,EVENT,P1,P2,P3,WAIT_TIME,SECONDS_IN_WAIT,STATE from 
 v$session , v$process where addr=paddr and event like '%IPC send completion 
 sync%' ;
 
 
3] - set the below event in that session and allow tracing to continue for 
 around 5 minutes.one can repeat it for around 3 SPIDS identified from above 
 sql.
 
 sqlplus /as sysdba
 oradebug setospid <SPID from above sql>
 oradebug unlimit
 oradebug errorstack level 3
 oradebug event=10402 trace name context forever, level 1
 oradebug event=10046 trace name context forever, level 12
 oradebug errorstack level 3
 
4] - set the event OFF in the session after 5 minutes..
 
 oradebug errorstack level 3
 oradebug event=10402 trace name context off
 oradebug event=10046 trace name context off
 oradebug errorstack level 3
 
5] - gather oswatcher at interval of 30 seconds.make sure to trace the 
 interconnect.

6] - gather AWR reports from all the rac instances at interval of 25 minutes.


=========================================================================================================================================================
########################################################################################################################################################
=========================================================================================================================================================
=========================================================================================================================================================
2. STORAGE DATA COLLECTION 
=======================

=======================================================================================
STORAGE DATA COLLECTION ASM
=======================================================================================

				Key : ASMLIB
				
				Please provide the following:

				ASM alert logs

1. Please execute the next commands and show me the output (from each RAC node)
==================================================================
$> cat /etc/*release
$> uname -a
$> rpm -qa |grep oracleasm
$> df -ha
$> ls -l /dev/oracleasm/disks
==================================================================

2. Check the discovery path from each node::
==================================================================

$> /etc/init.d/oracleasm status
$> /usr/sbin/oracleasm-discover
$> /usr/sbin/oracleasm-discover 'ORCL:*'
==================================================================

3. Please check if the ASMLIB devices can be accessed from each RAC node
==================================================================
$> /etc/init.d/oracleasm scandisks
$> /etc/init.d/oracleasm listdisks
$> /etc/init.d/oracleasm querydisk -p <each disk from previous output>
$> /etc/init.d/oracleasm querydisk -d <each disk from previous output>

==================================================================

4. Upload the next files from each RAC node:

=)> /var/log/messages
=)> /etc/sysconfig/oracleasm

5. Please show me the partition table from each node:
==================================================================
$> cat /proc/partitions
==================================================================

Thank you



=====================================================================
STORAGE DATA COLLECTION DNFS
=====================================================================


						=======================
						DISABLE DNFS
						=======================
						DNFS is enabled by default and been used automatically in 12C(DNFS is disabled in 11.2)
						because It has higher priority than regular NFS client.

						Please execute command and provide the output.
						$ pwd
						$ cd $ORACLE_HOME/lib
						ls -l *odm*

						To disable DHFS , shutdown the DB and do:
						(official way): cd $ORACLE_HOME/rdbms/lib; make -f ins_rdbms.mk dnfs_off ioracle
						(non-official): rename this file:
						Example:

						# ll $ORACLE_HOME/rdbms/lib/odm/
						total 100
						-rw-r--r--. 1 oracle oinstall 96003 Jul 16 23:12 libnfsodm12.so
						# cd $ORACLE_HOME/rdbms/lib/odm/
						# mv libnfsodm12.so libnfsodm12.so.OLD

						Now start the instance, and you should not see the Direct NFS lines in the
						alert.log

						This will work only when it is in this half/disabled/enabled state.
						Please follow the first method and use second method for debug/rediscovery
						purposes.

						
=========================================================================================================================================================
########################################################################################################################################################
=========================================================================================================================================================
=========================================================================================================================================================
RAC DATA COLLECTION
==================================================================



Would you please clarify me below queries to start troubleshooting.     
1. How many nodes in cluster..?     
      Output of   
				  a. crsctl stat res -t  
                  b. crsctl check crs  
                  c. ps -ef | grep init  
				  d. ps -ef | grep d.bin    
	  
2. Which node is having the issue..? 
   node name please..?
   
3. Get me exact issue date and time.     

4.How was the resource utilization on the system when crash happened?     
To check this, please get me OSW logs that covering issue time     
Note.301137.1 OS Watcher Black Box User Guide (Doc ID 301137.1)     

5. who -b command output from all the nodes.     

6. Was there any recent changes happened..?     

7.Did you/your sysadmin see any errors in the OS messages file?   
  Please upload sys logs that cover issue time    

8. Please upload the diag collection from ALL the nodes  . 
The crs log files using TFA:         

9. Upload database alert logs     
10. Please get me sys logs messages     
11. What is the current status.?     
12. What is the status of DIAGWAIT     crsctl get css diagwait     Regards,  Rakesh  
 

 

PLease upload the following information
Please upload diagcollection logs / TFA logs from all the nodes.

1) $GI_HOME/bin/tfactl diagcollect -from <date/time> -to <date/time>
where <date/time> is in "MMM/dd/yyyy hh:mm:ss" format; for example, "Jul/1/2014 21:00:00"
Specify the "from time" to be 4 hours before and the "to time" to be 4 hours after the time of error.
The TFA Collector will extract and collect the trace and log information written between the "from time" and "to time" from files.

Issue "$GI_HOME/bin/tfactl diagcollect -h" to get the help page for tfactl command.


Upload all files that TFA collector creates to SR.
Note 1513912.1 - TFA Collector - Tool for Enhanced Diagnostic Gathering and start TFA



CRS 10gR2/ 11gR1/ 11gR2 Diagnostic Collection Guide [Document 330358.1]
* For 11gR2
        o Execute <GRID_HOME>/bin/diagcollection.sh as Root user

2) Upload AWR reports from all instances of the problem database to SR.
      The time period for AWR report should not be greater than 1 hour and preferably 30 minutes during the time of performance problem.
      Get one AWR report just prior to the database hang started and another just after the database hang started (Get the AWR reports if they are available).


	  3) Upload ASH reports from all instances of the problem database to SR.
     ASH report can cover shorter time period like 10 minutes, so upload multiple ASH reports covering 10 minutes just prior to the hang and just after the hang.


4) Upload ADDM reports from all instances of the problem database to SR.


5) Set your environment to the GI home.
   opatch lsinventory

6)Set your environment to the RDBMS home.
   opatch lsinventory


7)Provide your OSW (OSWatcher) output, for the one hour timeframe leading up to and including the problem for EACH node.
   If you did not have OSWatcher running at the time of the instance hangs, you won't be able to provide the oswatcher .dat files.
 
   OSWatcher gathers OS data such as vmstat, iostat, netstat, mpstat, in files that each hold 1 hour of data.
   If you do not have OSWatcher running, then please set it up per OS Watcher Black Box User Guide (Doc ID 301137.1).
 

		5 Simple Steps to configurue OSW in ( probably even less than ) 5 min
		==================================== 
		1] Download OSW from DOC :

		2] untar the downloaded zip file
		   #tar xvf oswbb.tar

		3] get inside the archive folder of the OSBB
		   #pwd
		   /home/oracle/downloads/oswbb/archive
		   
		4] Start os watcher for 20 sec inetrval and 73 hours of retention
		   #./startOSWbb.sh  20 72
		   
		5] Stop OS watcher after required files are connected
		   ./stopOSWbb.sh

 
 We recommend configuring OSWatcher with an interval of 20 seconds and a retention of at least 2 days.
 Be sure to setup the private.net (traceroute of the private interconnects) when setting up OSWatcher.
 The OSWatcher tar file you downloaded includes an Exampleprivate.net. You can make your private.net file from the OS specific
 sample info within Exampleprivate.net.  Once you have created private.net be sure to give it execute permissions.


 The  relevant  notes are:
 OSWatcher Black Box (Includes: [Video]) (Doc ID 301137.1)
 OS Watcher User's Guide (Doc ID 1531223.1)
 OSWatcher Black Box Analyzer User Guide (Doc ID 461053.1)
 How To Start OSWatcher Black Box (OSWBB) Every System Boot (Doc ID 580513.1)

 
=========================================================================================================================================================
########################################################################################################################################################
=========================================================================================================================================================
=========================================================================================================================================================
INSTALL PATCH UPGRADE 
===========================================


			Below are the POST INSTALLATION steps:
			+++++++++++++++++++++
			For OJVM PSU Patch:
			+++++++++++++++++++++
			cd $ORACLE_HOME/sqlpatch/19877440
			sqlplus /nolog
			SQL> CONNECT / AS SYSDBA
			SQL> @postinstall.sql
			SQL> quit
			cd $ORACLE_HOME/rdbms/admin
			sqlplus /nolog
			SQL> CONNECT / AS SYSDBA
			SQL> @utlrp.sql
			+++++++++++++++++++++
			For Database PSU patch:
			+++++++++++++++++++++
			cd $ORACLE_HOME/rdbms/admin
			sqlplus /nolog
			SQL> CONNECT / AS SYSDBA
			SQL> STARTUP
			SQL> @catbundle.sql psu apply
			SQL> QUIT
			cd $ORACLE_HOME/rdbms/admin
			sqlplus /nolog
			SQL> CONNECT / AS SYSDBA
			SQL> @utlrp.sql


			





=========================================================================================================================================================
########################################################################################################################################################
=========================================================================================================================================================
=========================================================================================================================================================
OS DATA COLLECTION
==================================================================


			=======================================================
 			OS TRACING STRACE TRUSS TRUC
			=======================================================
				OS Process

				2. Get a strace for a process reporting this transient corruption:

				In order to do this steps, indentify the process at OS level that makes the backup

				2.1.Connect to RMAN
				$rman target /
				2.2.Get the spid for RMAN.
				$sqlplus / a sysdba
				SQL>set linesize 200
				SQL>select p.spid,substr(s.osuser,1,10) osuser,substr(s.username,1,8) username,substr(s.action,1,60) 
				rmaninfo from v$session s, v$process p where s.paddr=p.addr and s.osuser is not null and s.program like '%rman%' order by s.osuser;

				2.3. Execute the backup command in RMAN
				2.4. Re-execute the query in SQL*Plus and you will see in RMANINFO column something like " STARTED16"
				2.5. Get the spid and trace it

				$ strace -o backupdebug.log -t -p <SPID of Oracle server process>

				++++++++++++++++++++++++++++++++++
				ASMCMD TRACE
				
				$ strace -aefT -o /tmp/asmcmd.log asmcmd
				ASMCMD> -- run the command



				
				
				
				
				
				
				
	
==============================
DATABASE LOGS AND ASM LOGS
===============================
1]  Provide instance alert_{$ORACLE_SID}.log From ALL THE NODES  

2]  Provide instance alert_+ASM<n>.log From ALL THE NODES  

3]  Other modified traces from  ALL Problem Database instance of ALL the nodes

   - lmon, lmd*, lms*, ckpt, lgwr, lck*, dia*, lmhb(11g only), and all others traces that are modified around incident time.
 A quick way to identify all traces and tar them up is to use incident time with the following example:

  $ grep "2010-09-02 03" *.trc | awk -F: '{print $1}' | sort -u |xargs tar cvf trace.`hostname`.`date +%Y%m%d%H%M%S`.tar
  $ gzip trace*.tar

   For 11g+, execute the command in ${ORACLE_BASE}/diag/rdbms/$DBNAME/${ORACLE_SID}/trace to identify the list of files

4]   Other modified traces from  ALL Problem Database instance of ALL the nodes

   - lmon, lmd*, lms*, ckpt, lgwr, lck*, dia*, lmhb(11g only), and all others traces that are modified around incident time.
 A quick way to identify all traces and tar them up is to use incident time with the following example:

  $ grep "2010-09-02 03" *.trc | awk -F: '{print $1}' | sort -u |xargs tar cvf trace.`hostname`.`date +%Y%m%d%H%M%S`.tar
  $ gzip trace*.tar

   For 11g+, execute the command in ${ORACLE_BASE}/diag/asm/+asm<>/+ASM<>/trace to identify the list of files

   
5] Incident files/packages in alert.log at time of the incident
 
       <ORACLE_BASE>/rdbms/<DBNAME>/<DBINSTANCE>/incident/*
	    ${ORACLE_BASE}/diag/asm/+asm<>/+ASM<>/incident/*

6] OSwatcher logs Covering the issue time .			

7] Clusterware logs 
      CRS 10gR2/ 11gR1/ 11gR2 Diagnostic Collection Guide [Document 330358.1]
* For 11gR2
        o Execute <GRID_HOME>/bin/diagcollection.sh as Root user

				
				
			
				
		SQL> ALTER SYSTEM START ROLLING PATCH;

		
				
				
				
				
	
				
				
				
				
2). Provide us with the output of

# cat /etc/hosts
# nslookup <scan address>
# ifconfig -a
# oifcfg iflist -p -n
# oifcfg getif
# netstat -ni
# route
$ olsnodes

.
3). We need to see the following
Please show the command along with the output so we can separate the data
cd $GRID_HOME/bin
./crsctl stat res -t
./crsctl status resource -w  "TYPE = ora.scan_vip.type" -t
./crsctl stat res -w "TYPE = ora.scan_listener.type"
./srvctl config scan_listener
./lsnrctl status <<scan listener name>>
$GRID_HOME/bin/srvctl config scan
$GRID_HOME/bin/srvctl config scan_listener
$GRID_HOME/bin/srvctl status scan
$GRID_HOME/bin/srvctl status scan_listener
For the scan listener listening service status:
 $GRID_HOME/bin/lsnrctl status listener_scan<n>
         where <n> is from 1 to 3 if there are multiple scan_listeners
.
4). Upload the text version of the listener logs
The 11g listener log in text format is located here:$ORACLE_BASE/diag/tnslsnr/<your_host>/<listener_name>/trace/<listener_name>.log
.
5). Provide us with the following
<grid_home/bin/ crsctl stat res -t >crsctl.out
upload the crsctl.out file

				
				
$ setenv SRVM_TRACE true (or "export SRVM_TRACE=true")
$ setenv SRVM_TRACE_LEVEL 2 (or "export SRVM_TACE_LEVEL=2")

$./addNode.sh -debug -logLevel finest "CLUSTER_NEW_NODES={node3}" "CLUSTER_NEW_VIRTUAL_HOSTNAMES={node3-v}"		
				
============

=================


from db home oradim -ex oracle registry get OCR_LOC
from db home oradim -ex oracle registry get OLR_LOC

2]. Windows Events in txt format that covers incident timeframe:
  Start => All Programs => Administrative Tools => Event Viewer, choose "Application",  "Save Log File As" .txt format, then repeat same steps for "System" and "Secutiry" so you have three saved .txt files

) Please provide me with the settings for the following registry keys from -all- nodes:  

\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Session Manager\SubSystems\
\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Session Manager\Environment\

- open regedit and navigate to each of the above paths in turn.
- right click and select "Export"

- enter "File Name" and select "text" as "Save as Type". Do NOT save it as a .reg file! Do NOT upload .reg files to this SR! Upload only text exports to this SR.
Repeat above for all of the above registry keys.



2) Please provide the output of "tasklist" from all nodes

3) Please upload the Application and System Event logs from *ALL* nodes.
Note:  The Event Logs should be saved as TEXT files.  To do this, right click on the appropriate log in Event Viewer and choose 'Save Log File As'
In the 'Save as type'  box, choose TEXT. Simply specifying a file suffix of TEXT or TXT will not save the files in TEXT format.
You must ensure that you choose the correct TYPE in the "Save as Type" box. 
  
  
  
  
  
  
  Please collect and upload:

1. Full text version of ASM alert.log from all ASM instances in the cluster not just the instance that is reporting the error.
===========================================================================================
2. Please connect to each ASM instance, execute the next script on your ASM instances and then provide the output files (from each node since this is a RAC configuration):
===========================================================================================
spool asm_acfs_<asm_instance_name>.html
-- Version 11.2
SET MARKUP HTML ON
set echo on

set pagesize 200
alter session set nls_date_format='DD-MON-YYYY HH24:MI:SS';

select 'THIS ASM REPORT WAS GENERATED AT: ==)> ' , sysdate " " from dual;
select 'HOSTNAME ASSOCIATED WITH THIS ASM INSTANCE: ==)> ' , MACHINE " " from v$session where program like '%SMON%';
select * from v$asm_diskgroup;
SELECT * FROM V$ASM_DISK ORDER BY GROUP_NUMBER,DISK_NUMBER;
select * from V$ASM_ACFSSNAPSHOTS;
select * from V$ASM_ACFSVOLUMES;
select * from V$ASM_FILESYSTEM;
select * from V$ASM_VOLUME;
select * from V$ASM_VOLUME_STAT;
select * from v$version;
show parameter asm
show parameter cluster
show parameter instance_type
show parameter instance_name
show parameter spfile
show sga
spool off
exit
===========================================================================================
3. Please provide TFA diag collection info from all nodes.
Note 1513912.1 - TFA Collector - Tool for Enhanced Diagnostic Gathering
If TFA is not possible, please provide CRS diag collection instead as well as the DB alert log/traces from all nodes.
Note 330358.1 - CRS 10gR2/ 11gR1/ 11gR2  / 12cR1 Diagnostic Collection Guide

Options for collecting :
-
/u01/app/11.2.0/grid/tfa/bin/tfactl diagcollect -all -since 8h
  Trim and Zip all files updated in the last 8 hours as well as chmos/osw data
  from across the cluster and collect at the initiating node
-
/u01/app/11.2.0/grid/tfa/bin/tfactl diagcollect -for "Mar/2/2013 21:00:00"
  Trim and Zip all log files updated from 09:00 on March 2 to 09:00 on March 3
  (i.e. 12 hours before and after the time given) and collect at the initiating node


===========================================================================================
4. Show me the output of below commands from all nodes
===========================================================================================
$> asmcmd
ASMCMD> volstat
ASMCMD> volinfo -a

$> crsctl stat res -t
$> crsctl stat res -t -init


# /sbin/acfsutil registry -l
$ acfsdriverstate -orahome $GRID_HOME supported
$ acfsdriverstate -orahome $GRID_HOME version
$ acfsdriverstate -orahome $GRID_HOME installed
$ acfsdriverstate -orahome $GRID_HOME loaded

===========================================================================================

truss -eaf -p -o /tmp/rootupgrade.truss <NEW_GI_HOME>/rootupgrade.sh
truss -eaf -p -o /tmp/ohasd.truss 

script /tmp/rootupgrade
bash -x   <NEW_GI_HOME>/rootupgrade.sh
sqlplus / as sysasm
 


while true; do pid=$(pgrep 'ocssd.bin' | head -1); if [[ -n "$pid" ]]; then strace  -s 200 -o /tmp/ocssdstarce.log -vvtf -p "$pid"; break; fi; done &


while true; do pid=$(pgrep 'oraagent.bin' | head -1); if [[ -n "$pid" ]]; then strace  -s 2000 -o /tmp/ohasdoraagentstarce.log -vvtf -p "$pid"; break; fi; done &


while true; do pid=$(pgrep 'ocssd.bin' | head -1); if [[ -n "$pid" ]]; then truss -eaf -p -o /tmp/ohasdstace.log "$pid"; break; fi; done
while true; do pid=$(pgrep 'oraagent.bin' | head -1); if [[ -n "$pid" ]]; then truss -eaf -p -o /tmp/ohasdoraagentstace.log "$pid"; break; fi; done

while true; do pid=$(ps -ef |grep -v grep | grep 'ohasd.bin' | awk '{print $2}'); if [[ -n "$pid" ]]; then truss -eaf -o /tmp/ohasdstace.log -p "$pid"; break; fi; done
while true; do pid=$(ps -ef |grep -v grep | grep 'oraagent.bin' | awk '{print $2}'); if [[ -n "$pid" ]]; then truss -eaf -o /tmp/ohasdoraagentstace.log -p "$pid"; break; fi; done



 ps -ef | grep ocssd.bin | head -1 | awk '{print $2}'
 
while true; do pid=$( ps -ef | grep 'ocssd.bin' | head -1 | awk '{print $2}'); if [[ -n "$pid" ]]; then truss -eaf -o /tmp/ocssdtruss -p "$pid"; break; fi; done


Please upload the InstallActions.* files  (log, err, out) from your latest install attempt.
Logs can be found in <ORACLE_HOME>/cfgtoollogs/oui

<ORACLE_HOME>/cfgtoollogs/oui/InstallActions<timestamp>.log
<ORACLE_HOME>/cfgtoollogs/oui/oraInstall<timestamp>.out
<ORACLE_HOME>/cfgtoollogs/oui/oraInstall<timestamp>.err


<GI_HOME>/bin/kfod op=patchlvl
<GI_HOME>/bin/kfod op=patches
<GI_HOME>/cfgtoollogs/opatchauto/<patch>/
<GI_HOME>/cfgtoollogs/opatch/ and
<DB_HOME>/cfgtoollogs/opatch/


truss -eafo /tmp/start_truss.out crsctl start crs
truss -rall -wall -eafo /tmp/startup 
truss -eafo /tmp/sqltruss sqlplus / as sysdba 

truss -eafo /tmp/ocrconfig.out  


pargs -e 8091




-------------------------------------
Bug 11866171 - Linux: Enable Crashdump when rebooting the machine (Doc ID 11866171.8)

This bug fix is to enable OS crash dumps during the clusterware reboots on Linux platforms. 
 
Once the patch is applied, please run the following commands as root user to turn on the crash dump:
 
 crsctl modify type ora.cssd.type -attr "ATTRIBUTE=REBOOT_OPTS, TYPE=string, DEFAULT_VALUE=,FLAGS=CONFIG" -init
 crsctl modify type ora.cssdmonitor.type -attr "ATTRIBUTE=REBOOT_OPTS, TYPE=string, DEFAULT_VALUE=,FLAGS=CONFIG" -init
 crsctl modify res ora.cssd -attr "REBOOT_OPTS=CRASHDUMP" -init
 crsctl modify res ora.cssdmonitor -attr "REBOOT_OPTS=CRASHDUMP" -init
 
To disable the crash dump, as root user:
 crsctl modify res ora.cssd -attr "REBOOT_OPTS=" -init
 crsctl modify res ora.cssdmonitor -attr "REBOOT_OPTS=" -init
 
Note that this needs to be done on all nodes, also ensure OS kdump works fine:
 
note 452067.1 - How to Configure "kdump" on Oracle Linux 5 
note 1507025.1 - How to Configure "kdump" on Oracle Linux 6 
 
For AIX, use:
 
note 17208793.8 - Producing an AIX system crash dumps on clusterware initiated reboots

--------------------------------------------------------------------



	Location of runInstaller(OUI) log/trace (Doc ID 1629698.1)



Additional tracing produced in the OUI log file can be accomplished using:

./runInstaller -logLevel detailed /-J-DTRACING.ENABLED=true -J-DTRACING.LEVEL=2/

Run as below

script /tmp/oui_tracing.txt

./runInstaller -logLevel detailed /-J-DTRACING.ENABLED=true -J-DTRACING.LEVEL=2/
exit


Please Upload

1. /tmp/oui_tracing.txt,<<Central_inv_logs>>/logs,DBCA log folder
2. mount option of NFS4


Refer How to trace OUI (Doc ID 269837.1).


date chronyc tracking


Please upload the output of the following:
$ opatch prereq CheckComponents -ph $PBASE/<PATCHNUMBER>  -oh <GRID_HOME> 

$ opatch prereq CheckApplicable -ph $PBASE/<PATCHNUMBER>  -oh <GRID_HOME> 

$ opatch prereq CheckConflictAgainstOH -ph $PBASE/<PATCHNUMBER>  -oh <GRID_HOME> 

$ opatch prereq CheckComponents -ph $PBASE/<PATCHNUMBER>  -oh <GRID_HOME>

$ opatch prereq CheckApplicable -ph $PBASE/<PATCHNUMBER>  -oh <GRID_HOME>

$ opatch prereq CheckConflictAgainstOH -ph $PBASE/<PATCHNUMBER>  -oh <GRID_HOME>

-----------------------------------------------------------------------------------------



ocrdump -local -stdout | grep -A3 SYSTEM.css | grep GROUP

truss -rall -wall  -eafo /tmp/runinstaller.out  ./runInstaller -debug -J-DTRACING.ENABLED=true -J-DTRACING.LEVEL=2 -J-DSRVM_TRACE_LEVEL=2 -J-DFULLTRACE

lfsdiag.sql collection




Please Dettach the Old Oracle Home which are not used from the Central Inventory :

cd $OHOME/oui/bin
./runInstaller -detachhome ORACLE_HOME=$OHOME ORACLE_HOME_NAME=$OHOMENAME
Example :

cd  /orabin/grid/product/12.1.0.2/grid/oui/bin
./runInstaller -detachhome ORACLE_HOME=/orabin/grid/product/11.2.0/grid ORACLE_HOME_NAME=Ora11g_gridinfrahome1          
./runInstaller -detachhome ORACLE_HOME= /orabin/agent10g  ORACLE_HOME_NAME=agent10g







bash-4.2# cat  prostack.sh 
#!/bin/sh 
 
if [ $# -ne 1 ] 
then 
   echo "Usage procstack  <processid>, Enter OCSSD process id"; 
   exit; 
fi 
 
a=1 
rm -rf CSSD_$1.stacktrace.txt 
rm -rf CSSD_$1.stacktracerr.txt 
 
while [ $a -le 10000 ] 
do 
   echo "Iteration = $a" >> CSSD_$1.stacktrace.txt 
   DATE=`date` 
   echo "Time stamp - $DATE" >> CSSD_$1.stacktrace.txt 
   echo "PROCESS STACK OF CSSD threads :-" >> CSSD_$1.stacktrace.txt 
   procstack $1 >> CSSD_$1.stacktrace 2> CSSD_$1.stacktracerr.txt 
   a=`expr $a + 1` 
  sleep 5; 
done 